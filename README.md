# Save the README content to a text file

readme_content = """# ğŸŒ¦ï¸ Weather Data ETL Pipeline using Apache Airflow & Python

## ğŸ“˜ Overview
This project implements a complete **ETL (Extract, Transform, Load)** data pipeline using **Python** and **Apache Airflow**, designed to automate the collection and storage of **real-time weather data** from the **OpenWeather API** for major **Indian cities**.  

The pipeline extracts live data, performs feature engineering and transformations, and loads the cleaned data into a **MySQL Data Warehouse** for analysis, visualization, or downstream processing.

---

## ğŸ§° Tech Stack

| Component | Technology |
|------------|-------------|
| **Workflow Orchestration** | Apache Airflow |
| **Programming Language** | Python |
| **Data Source** | OpenWeather API |
| **Database** | MySQL |
| **Libraries Used** | requests, pandas, mysql-connector, sqlalchemy |
| **Storage Output** | MySQL Tables + CSV Backup |

---

## ğŸ§© Project Architecture

                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        Apache Airflow         â”‚
                â”‚   (Scheduler & Orchestrator)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚          Extraction           â”‚
                â”‚  â†’ Fetch data from OpenWeatherâ”‚
                â”‚    API for Indian cities      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚         Transformation        â”‚
                â”‚  â†’ Clean, convert, derive new  â”‚
                â”‚    metrics (temp range, etc.)  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚            Load              â”‚
                â”‚  â†’ Store into MySQL tables:   â”‚
                â”‚    city & weather_fact        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## ğŸ—ï¸ Project Structure

Weather-Data-ETL-Pipeline-using-Apache-Airflow/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ weather_dag.py                 # Airflow DAG definition
â”‚
â”œâ”€â”€ etl/
â”‚   â””â”€â”€ etl_pipeline.py                # ETL class (extraction, transformation, loading)
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ api_config.json                # API key and URL
â”‚   â””â”€â”€ db_config.json                 # MySQL credentials
â”‚
â”œâ”€â”€ Indian_City_Weather.csv            # Output CSV (optional local backup)
â”‚
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”‚
â””â”€â”€ README.md

---

## âš™ï¸ Workflow Explanation

### 1ï¸âƒ£ Extraction Phase
Fetches real-time weather data for each city from OpenWeather API, normalizes it into structured form, and stores a CSV backup.

### 2ï¸âƒ£ Transformation Phase
Performs feature engineering and conversions (Â°Câ†’Â°F, hPaâ†’atm), fills missing values, and adds new columns like 'temp_range', 'humidity_category', 'pressure_category'petc.

### 3ï¸âƒ£ Loading Phase
Inserts data into MySQL tables (`city`, `weather_fact`) with foreign key mapping.

---

## ğŸš€ Setup & Execution

1ï¸âƒ£ Clone Repository
git clone https://github.com/<your-username>/Weather-Data-ETL-Pipeline-using-Apache-Airflow.git

2ï¸âƒ£ Create Virtual Environment
python -m venv venv
source venv/bin/activate    # Linux/Mac
venv\\Scripts\\activate       # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Configure Files
- Add API key in config/api_config.json
- Add MySQL credentials in config/db_config.json

5ï¸âƒ£ Initialize and Start Airflow
airflow db init
airflow webserver -p 8080
airflow scheduler

6ï¸âƒ£ Trigger the DAG
Open http://localhost:8080 and start the DAG.

---

## ğŸ§¾ Future Enhancements
- Integrate Airflow Sensors for data validation  
- Add AWS S3 or Snowflake as warehouse  
- Build Streamlit dashboard for visualization  
- Implement incremental loading

---

## ğŸ‘¨â€ğŸ’» Author
**Kothandaramar Sivakumar**  
ğŸ“§ skramar1999@gmail.com

---

## ğŸªª License
Licensed under the MIT License.
"""
